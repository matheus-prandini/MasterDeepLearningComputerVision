{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Fashion-MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "R3c7kkSjNdGX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Fashion-MNIST with Keras"
      ]
    },
    {
      "metadata": {
        "id": "FfT6FxeaNl3y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 - Load and Preprocessing data"
      ]
    },
    {
      "metadata": {
        "id": "BXR3cxGINcRC",
        "colab_type": "code",
        "outputId": "7ebcaac1-2754-4ab4-d3d4-041875d035fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.regularizers import l2\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.utils import np_utils\n",
        "import keras\n",
        "\n",
        "# loads the Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test)  = fashion_mnist.load_data()\n",
        "\n",
        "# Lets store the number of rows and columns\n",
        "img_rows = x_train[0].shape[0]\n",
        "img_cols = x_train[0].shape[1]\n",
        "\n",
        "# Getting our date in the right 'shape' needed for Keras\n",
        "# We need to add a 4th dimenion to our date thereby changing our\n",
        "# Our original image shape of (60000,28,28) to (60000,28,28,1)\n",
        "x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "\n",
        "# store the shape of a single image \n",
        "input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "# change our image type to float32 data type\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "# Normalize our data by changing the range from (0 to 255) to (0 to 1)\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "# Now we one hot encode outputs\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "num_classes = y_test.shape[1]\n",
        "num_pixels = x_train.shape[1] * x_train.shape[2]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 3us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 1s 0us/step\n",
            "(60000, 28, 28, 1)\n",
            "(10000, 28, 28, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QrHCoTqcPZbI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Building, Training and Testing the model without batch normalization"
      ]
    },
    {
      "metadata": {
        "id": "y_9Lfzg7Pb68",
        "colab_type": "code",
        "outputId": "feed2e00-fb6e-49d5-fbdc-2d2690c8cf06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import backend as K\n",
        "\n",
        "# Training Parameters\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = keras.optimizers.Adadelta(),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "udX1TOP9Qawt",
        "colab_type": "code",
        "outputId": "6513f719-db06-4c99-9588-9274ef629798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 10s 173us/step - loss: 0.5772 - acc: 0.7943 - val_loss: 0.3689 - val_acc: 0.8655\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.3683 - acc: 0.8690 - val_loss: 0.3214 - val_acc: 0.8828\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.3137 - acc: 0.8871 - val_loss: 0.2939 - val_acc: 0.8935\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.2828 - acc: 0.8979 - val_loss: 0.2792 - val_acc: 0.8948\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 9s 142us/step - loss: 0.2578 - acc: 0.9085 - val_loss: 0.2485 - val_acc: 0.9099\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.2392 - acc: 0.9138 - val_loss: 0.2434 - val_acc: 0.9117\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.2240 - acc: 0.9183 - val_loss: 0.2497 - val_acc: 0.9115\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.2111 - acc: 0.9244 - val_loss: 0.2309 - val_acc: 0.9152\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.1994 - acc: 0.9268 - val_loss: 0.2225 - val_acc: 0.9203\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 9s 143us/step - loss: 0.1892 - acc: 0.9311 - val_loss: 0.2138 - val_acc: 0.9251\n",
            "Test loss: 0.2137680757880211\n",
            "Test accuracy: 0.9251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "IzvNokKvQi6q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Building, Training and Testing the model with batch normalization"
      ]
    },
    {
      "metadata": {
        "id": "FI5mB5b6Qokj",
        "colab_type": "code",
        "outputId": "6df86289-d4ca-42d4-d9cb-97f149c5588d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import mnist\n",
        "from keras.utils import np_utils\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras import backend as K\n",
        "\n",
        "# Training Parameters\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "# create model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = keras.optimizers.Adadelta(),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,200,778\n",
            "Trainable params: 1,200,330\n",
            "Non-trainable params: 448\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m-SznnB8RIT6",
        "colab_type": "code",
        "outputId": "f1f12e48-dce6-43cd-b4e9-094dbc662c2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 13s 213us/step - loss: 0.4563 - acc: 0.8442 - val_loss: 0.3336 - val_acc: 0.8820\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 0.2861 - acc: 0.8979 - val_loss: 0.2515 - val_acc: 0.9080\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.2418 - acc: 0.9141 - val_loss: 0.2463 - val_acc: 0.9108\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 12s 197us/step - loss: 0.2084 - acc: 0.9250 - val_loss: 0.2309 - val_acc: 0.9175\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 12s 198us/step - loss: 0.1867 - acc: 0.9329 - val_loss: 0.2225 - val_acc: 0.9178\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 12s 203us/step - loss: 0.1685 - acc: 0.9397 - val_loss: 0.2316 - val_acc: 0.9200\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 0.1576 - acc: 0.9428 - val_loss: 0.2238 - val_acc: 0.9231\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 12s 203us/step - loss: 0.1442 - acc: 0.9479 - val_loss: 0.2167 - val_acc: 0.9241\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 12s 201us/step - loss: 0.1344 - acc: 0.9518 - val_loss: 0.2356 - val_acc: 0.9234\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.1243 - acc: 0.9552 - val_loss: 0.2169 - val_acc: 0.9269\n",
            "Test loss: 0.21694599533379078\n",
            "Test accuracy: 0.9269\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}