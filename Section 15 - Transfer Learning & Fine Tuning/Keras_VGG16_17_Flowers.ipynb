{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_VGG16_17-Flowers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OEkOWbB29Pvl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Flower Classifier with VGG16 in Keras"
      ]
    },
    {
      "metadata": {
        "id": "KbdCcQjQ9VV2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 - Downloading dataset from Google Drive and Preprocessing data"
      ]
    },
    {
      "metadata": {
        "id": "OdZplia7BT9t",
        "colab_type": "code",
        "outputId": "bb5928fd-1a12-46d3-b1a8-a774c932527b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Cgo2M7zTBhW2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "\n",
        "local_tar_gz = '/content/drive/My Drive/17-flowers.tar.gz'\n",
        "tar_gz_ref = tarfile.open(local_tar_gz, \"r:gz\")\n",
        "tar_gz_ref.extractall('/tmp')\n",
        "tar_gz_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vCT0ZS8UBmyT",
        "colab_type": "code",
        "outputId": "1388f731-c3d5-4be8-ab00-29d8884f3eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Paths to training and validation data\n",
        "train_data_dir = '/tmp/17_flowers/train'\n",
        "validation_data_dir = '/tmp/17_flowers/validation'\n",
        "\n",
        "# VGG16 was designed to work on 224 x 224 pixel input images sizes\n",
        "img_rows = 224\n",
        "img_cols = 224 \n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "# Change the batchsize according to your system RAM\n",
        "train_batchsize = 16\n",
        "val_batchsize = 10\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=val_batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1190 images belonging to 17 classes.\n",
            "Found 170 images belonging to 17 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1QMetq68CcqP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Loading VGG16 model"
      ]
    },
    {
      "metadata": {
        "id": "HClGjTC2Cfpe",
        "colab_type": "code",
        "outputId": "23c9f993-babd-42cc-9c2e-53f455510949",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "#Loads the VGG16 model \n",
        "vgg16 = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-E8B0E1CCpRm",
        "colab_type": "code",
        "outputId": "117ae479-cdff-486e-966b-1efb1be4bad8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(vgg16.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D True\n",
            "2 Conv2D True\n",
            "3 MaxPooling2D True\n",
            "4 Conv2D True\n",
            "5 Conv2D True\n",
            "6 MaxPooling2D True\n",
            "7 Conv2D True\n",
            "8 Conv2D True\n",
            "9 Conv2D True\n",
            "10 MaxPooling2D True\n",
            "11 Conv2D True\n",
            "12 Conv2D True\n",
            "13 Conv2D True\n",
            "14 MaxPooling2D True\n",
            "15 Conv2D True\n",
            "16 Conv2D True\n",
            "17 Conv2D True\n",
            "18 MaxPooling2D True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VWu_oMXICvFz",
        "colab_type": "code",
        "outputId": "4ee9b609-b0ed-41ad-e78e-c775dc1e34a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# Re-loads the VGG16 model without the top or FC layers\n",
        "vgg16 = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))\n",
        "\n",
        "# Here we freeze the last 4 layers \n",
        "# Layers are set to trainable as True by default\n",
        "for layer in vgg16.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(vgg16.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mutgaJ4vDDXP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def addTopModel(bottom_model, num_classes, D=256):\n",
        "    \"\"\"creates the top or head of the model that will be \n",
        "    placed ontop of the bottom layers\"\"\"\n",
        "    top_model = bottom_model.output\n",
        "    top_model = Flatten(name = \"flatten\")(top_model)\n",
        "    top_model = Dense(D, activation = \"relu\")(top_model)\n",
        "    top_model = Dropout(0.3)(top_model)\n",
        "    top_model = Dense(num_classes, activation = \"softmax\")(top_model)\n",
        "    return top_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sVFyRdVTDHsT",
        "colab_type": "code",
        "outputId": "643d74df-0ef0-400d-8370-c4419698e930",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 955
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "\n",
        "num_classes = 17\n",
        "\n",
        "FC_Head = addTopModel(vgg16, num_classes)\n",
        "\n",
        "model = Model(inputs=vgg16.input, outputs=FC_Head)\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 25088)             0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               6422784   \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 17)                4369      \n",
            "=================================================================\n",
            "Total params: 21,141,841\n",
            "Trainable params: 6,427,153\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b0gbOs86ECH8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 - Training and Testing the model"
      ]
    },
    {
      "metadata": {
        "id": "ECizABNLEE2z",
        "colab_type": "code",
        "outputId": "89002db4-163d-4116-dd44-2645ca4aab8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "                   \n",
        "checkpoint = ModelCheckpoint(\"/flowers_vgg_v1.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 3,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "callbacks = [earlystop, checkpoint]\n",
        "\n",
        "# Note we use a very small learning rate \n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = RMSprop(lr = 0.001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "nb_train_samples = 1190\n",
        "nb_validation_samples = 170\n",
        "epochs = 25\n",
        "batch_size = 16\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size)\n",
        "\n",
        "model.save(\"/flowers_vgg_1.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "74/74 [==============================] - 29s 388ms/step - loss: 14.8009 - acc: 0.0642 - val_loss: 12.8184 - val_acc: 0.0800\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 12.81843, saving model to /flowers_vgg_v1.h5\n",
            "Epoch 2/25\n",
            "74/74 [==============================] - 25s 335ms/step - loss: 5.4151 - acc: 0.2066 - val_loss: 0.8889 - val_acc: 0.7300\n",
            "\n",
            "Epoch 00002: val_loss improved from 12.81843 to 0.88886, saving model to /flowers_vgg_v1.h5\n",
            "Epoch 3/25\n",
            "74/74 [==============================] - 25s 343ms/step - loss: 1.9802 - acc: 0.3761 - val_loss: 1.5601 - val_acc: 0.5600\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.88886\n",
            "Epoch 4/25\n",
            "74/74 [==============================] - 26s 345ms/step - loss: 1.5598 - acc: 0.5093 - val_loss: 0.5587 - val_acc: 0.8400\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.88886 to 0.55869, saving model to /flowers_vgg_v1.h5\n",
            "Epoch 5/25\n",
            "74/74 [==============================] - 25s 344ms/step - loss: 1.3004 - acc: 0.5923 - val_loss: 0.8136 - val_acc: 0.7200\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.55869\n",
            "Epoch 6/25\n",
            "74/74 [==============================] - 26s 347ms/step - loss: 1.2520 - acc: 0.6199 - val_loss: 0.5736 - val_acc: 0.8400\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.55869\n",
            "Epoch 7/25\n",
            "74/74 [==============================] - 26s 349ms/step - loss: 1.0776 - acc: 0.6689 - val_loss: 0.7337 - val_acc: 0.8000\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.55869\n",
            "Epoch 00007: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nr71OxSpFIL_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 - Trying to work with smaller images"
      ]
    },
    {
      "metadata": {
        "id": "HW09fQTrFLJQ",
        "colab_type": "code",
        "outputId": "e4d87856-d5af-4aed-bb99-ab2b51ad7f22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "\n",
        "# Setting the input size now to 64 x 64 pixel \n",
        "img_rows = 64\n",
        "img_cols = 64 \n",
        "\n",
        "# Re-loads the VGG16 model without the top or FC layers\n",
        "vgg16 = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))\n",
        "\n",
        "# Here we freeze the last 4 layers \n",
        "# Layers are set to trainable as True by default\n",
        "for layer in vgg16.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Let's print our layers \n",
        "for (i,layer) in enumerate(vgg16.layers):\n",
        "    print(str(i) + \" \"+ layer.__class__.__name__, layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 InputLayer False\n",
            "1 Conv2D False\n",
            "2 Conv2D False\n",
            "3 MaxPooling2D False\n",
            "4 Conv2D False\n",
            "5 Conv2D False\n",
            "6 MaxPooling2D False\n",
            "7 Conv2D False\n",
            "8 Conv2D False\n",
            "9 Conv2D False\n",
            "10 MaxPooling2D False\n",
            "11 Conv2D False\n",
            "12 Conv2D False\n",
            "13 Conv2D False\n",
            "14 MaxPooling2D False\n",
            "15 Conv2D False\n",
            "16 Conv2D False\n",
            "17 Conv2D False\n",
            "18 MaxPooling2D False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9ECtph5AFRgJ",
        "colab_type": "code",
        "outputId": "57773eac-4bc7-49e0-e1d3-50ef6d8eb46e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.applications import VGG16\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_data_dir = '/tmp/17_flowers/train'\n",
        "validation_data_dir = '/tmp/17_flowers/validation'\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=20,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "# Change the batchsize according to your system RAM\n",
        "train_batchsize = 16\n",
        "val_batchsize = 10\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=train_batchsize,\n",
        "        class_mode='categorical')\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=val_batchsize,\n",
        "        class_mode='categorical',\n",
        "        shuffle=False)\n",
        "\n",
        "# Re-loads the VGG16 model without the top or FC layers\n",
        "vgg16 = VGG16(weights = 'imagenet', \n",
        "                 include_top = False, \n",
        "                 input_shape = (img_rows, img_cols, 3))\n",
        "\n",
        "# Freeze layers\n",
        "for layer in vgg16.layers:\n",
        "    layer.trainable = False\n",
        "    \n",
        "# Number of classes in the Flowers-17 dataset\n",
        "num_classes = 17\n",
        "\n",
        "FC_Head = addTopModel(vgg16, num_classes)\n",
        "\n",
        "model = Model(inputs=vgg16.input, outputs=FC_Head)\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1190 images belonging to 17 classes.\n",
            "Found 170 images belonging to 17 classes.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_7 (InputLayer)         (None, 64, 64, 3)         0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, 64, 64, 64)        1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, 32, 32, 128)       147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, 16, 16, 256)       295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, 16, 16, 256)       590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, 8, 8, 256)         0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, 8, 8, 512)         1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               524544    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 17)                4369      \n",
            "=================================================================\n",
            "Total params: 15,243,601\n",
            "Trainable params: 528,913\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-SwlzaRFFd2I",
        "colab_type": "code",
        "outputId": "6bbf2077-a165-4dc3-ae1b-ff906a8536c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1059
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "                   \n",
        "checkpoint = ModelCheckpoint(\"/flowers_vgg_64.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 5,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
        "                              factor = 0.2,\n",
        "                              patience = 3,\n",
        "                              verbose = 1,\n",
        "                              min_delta = 0.00001)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\n",
        "\n",
        "# Note we use a very small learning rate \n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = RMSprop(lr = 0.0001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "nb_train_samples = 1190\n",
        "nb_validation_samples = 170\n",
        "epochs = 25\n",
        "batch_size = 32\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size)\n",
        "\n",
        "model.save(\"/flowers_vgg_64.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "37/37 [==============================] - 5s 148ms/step - loss: 2.8637 - acc: 0.1014 - val_loss: 2.5965 - val_acc: 0.2000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 2.59648, saving model to /flowers_vgg_64.h5\n",
            "Epoch 2/25\n",
            "37/37 [==============================] - 4s 103ms/step - loss: 2.6371 - acc: 0.1639 - val_loss: 2.2182 - val_acc: 0.5800\n",
            "\n",
            "Epoch 00002: val_loss improved from 2.59648 to 2.21820, saving model to /flowers_vgg_64.h5\n",
            "Epoch 3/25\n",
            "37/37 [==============================] - 5s 131ms/step - loss: 2.4665 - acc: 0.2421 - val_loss: 2.3138 - val_acc: 0.3400\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 2.21820\n",
            "Epoch 4/25\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 2.3065 - acc: 0.3226 - val_loss: 2.0996 - val_acc: 0.4000\n",
            "\n",
            "Epoch 00004: val_loss improved from 2.21820 to 2.09964, saving model to /flowers_vgg_64.h5\n",
            "Epoch 5/25\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 2.2280 - acc: 0.2900 - val_loss: 1.9944 - val_acc: 0.5200\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.09964 to 1.99442, saving model to /flowers_vgg_64.h5\n",
            "Epoch 6/25\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 2.0685 - acc: 0.3919 - val_loss: 1.8443 - val_acc: 0.5600\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.99442 to 1.84427, saving model to /flowers_vgg_64.h5\n",
            "Epoch 7/25\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 1.9934 - acc: 0.3976 - val_loss: 2.0148 - val_acc: 0.4400\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.84427\n",
            "Epoch 8/25\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 1.8487 - acc: 0.4611 - val_loss: 1.8245 - val_acc: 0.5200\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.84427 to 1.82453, saving model to /flowers_vgg_64.h5\n",
            "Epoch 9/25\n",
            "37/37 [==============================] - 5s 133ms/step - loss: 1.8284 - acc: 0.4651 - val_loss: 1.2330 - val_acc: 0.7800\n",
            "\n",
            "Epoch 00009: val_loss improved from 1.82453 to 1.23304, saving model to /flowers_vgg_64.h5\n",
            "Epoch 10/25\n",
            "37/37 [==============================] - 5s 134ms/step - loss: 1.7643 - acc: 0.4932 - val_loss: 1.8299 - val_acc: 0.3800\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.23304\n",
            "Epoch 11/25\n",
            "37/37 [==============================] - 5s 132ms/step - loss: 1.7464 - acc: 0.4804 - val_loss: 1.5071 - val_acc: 0.5800\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 1.23304\n",
            "Epoch 12/25\n",
            "37/37 [==============================] - 5s 135ms/step - loss: 1.6175 - acc: 0.5084 - val_loss: 1.2717 - val_acc: 0.6200\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 1.23304\n",
            "\n",
            "Epoch 00012: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
            "Epoch 13/25\n",
            "37/37 [==============================] - 5s 127ms/step - loss: 1.6052 - acc: 0.5101 - val_loss: 1.2945 - val_acc: 0.7000\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.23304\n",
            "Epoch 14/25\n",
            "37/37 [==============================] - 5s 136ms/step - loss: 1.5891 - acc: 0.5507 - val_loss: 1.6512 - val_acc: 0.4600\n",
            "Restoring model weights from the end of the best epoch\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 1.23304\n",
            "Epoch 00014: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}