{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras_Emotion_Detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "w9O6dOLxywZG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Using my own LVGG for Emotion Detection"
      ]
    },
    {
      "metadata": {
        "id": "7Lq8shTH0Tgq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 - Downloading and Extracting dataset from Drive"
      ]
    },
    {
      "metadata": {
        "id": "ZKMFZIq3yc_Z",
        "colab_type": "code",
        "outputId": "7c012622-d94c-460e-ad1b-0df248c05ab4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Wmv6-xWk0i9s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/content/drive/My Drive/fer2013.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lj5NbH526oe1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Preprocessing data and creating generators"
      ]
    },
    {
      "metadata": {
        "id": "NZyVkVsO6tr0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "160e5949-72d2-4f91-9112-5cf93c4d3bab"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "\n",
        "num_classes = 7\n",
        "img_rows, img_cols = 48, 48\n",
        "batch_size = 16\n",
        "\n",
        "train_data_dir = '/tmp/fer2013/train'\n",
        "validation_data_dir = '/tmp/fer2013/validation'\n",
        "\n",
        "# Let's use some data augmentaiton \n",
        "train_datagen = ImageDataGenerator(\n",
        "      rescale=1./255,\n",
        "      rotation_range=30,\n",
        "      shear_range=0.3,\n",
        "      zoom_range=0.3,\n",
        "      width_shift_range=0.4,\n",
        "      height_shift_range=0.4,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        " \n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        " \n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        color_mode = 'grayscale',\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True)\n",
        " \n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        validation_data_dir,\n",
        "        color_mode = 'grayscale',\n",
        "        target_size=(img_rows, img_cols),\n",
        "        batch_size=batch_size,\n",
        "        class_mode='categorical',\n",
        "        shuffle=True)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 3589 images belonging to 7 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xDBIedu67FQi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 - Building model"
      ]
    },
    {
      "metadata": {
        "id": "_mrUpW467HoW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1372
        },
        "outputId": "96f45f6a-0b7a-4688-844f-65fe342b9c6a"
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.layers.advanced_activations import ELU\n",
        "from keras.layers.core import Activation, Flatten, Dropout, Dense\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# First CONV-ReLU Layer\n",
        "model.add(Conv2D(64, (3, 3), padding = 'same', input_shape = (img_rows, img_cols, 1)))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Second CONV-ReLU Layer\n",
        "model.add(Conv2D(64, (3, 3), padding = \"same\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Max Pooling with Dropout \n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# 3rd set of CONV-ReLU Layers\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 4th Set of CONV-ReLU Layers\n",
        "model.add(Conv2D(128, (3, 3), padding=\"same\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Max Pooling with Dropout \n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# 5th Set of CONV-ReLU Layers\n",
        "model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# 6th Set of CONV-ReLU Layers\n",
        "model.add(Conv2D(256, (3, 3), padding=\"same\"))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "# Max Pooling with Dropout \n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# First set of FC or Dense Layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Second set of FC or Dense Layers\n",
        "model.add(Dense(256))\n",
        "model.add(Activation('elu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Final Dense Layer\n",
        "model.add(Dense(num_classes))\n",
        "model.add(Activation(\"softmax\"))\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_15 (Conv2D)           (None, 48, 48, 64)        640       \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_16 (Conv2D)           (None, 48, 48, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_17 (Conv2D)           (None, 24, 24, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "conv2d_18 (Conv2D)           (None, 24, 24, 128)       147584    \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 12, 12, 256)       295168    \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 12, 12, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 12, 12, 256)       1024      \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 12, 12, 256)       590080    \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 12, 12, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_24 (Batc (None, 12, 12, 256)       1024      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 6, 6, 256)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 256)               2359552   \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 256)               65792     \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 7)                 1799      \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 7)                 0         \n",
            "=================================================================\n",
            "Total params: 3,577,031\n",
            "Trainable params: 3,574,215\n",
            "Non-trainable params: 2,816\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "30pnyPsp8EP9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and Testing model"
      ]
    },
    {
      "metadata": {
        "id": "vkWJB5Wk8HB3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1441
        },
        "outputId": "33a6d186-3215-491e-93ef-41905aa0bbc3"
      },
      "cell_type": "code",
      "source": [
        "from keras.optimizers import RMSprop, SGD, Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "                     \n",
        "checkpoint = ModelCheckpoint(\"emotion_lvgg.h5\",\n",
        "                             monitor=\"val_loss\",\n",
        "                             mode=\"min\",\n",
        "                             save_best_only = True,\n",
        "                             verbose=1)\n",
        "\n",
        "earlystop = EarlyStopping(monitor = 'val_loss', \n",
        "                          min_delta = 0, \n",
        "                          patience = 5,\n",
        "                          verbose = 1,\n",
        "                          restore_best_weights = True)\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
        "\n",
        "# we put our call backs into a callback list\n",
        "callbacks = [earlystop, checkpoint, reduce_lr]\n",
        "\n",
        "# We use a very small learning rate \n",
        "model.compile(loss = 'categorical_crossentropy',\n",
        "              optimizer = Adam(lr=0.001),\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "nb_train_samples = 28273\n",
        "nb_validation_samples = 3534\n",
        "epochs = 20\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch = nb_train_samples // batch_size,\n",
        "    epochs = epochs,\n",
        "    callbacks = callbacks,\n",
        "    validation_data = validation_generator,\n",
        "    validation_steps = nb_validation_samples // batch_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "1767/1767 [==============================] - 115s 65ms/step - loss: 2.0696 - acc: 0.2005 - val_loss: 1.8830 - val_acc: 0.2089\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.88296, saving model to emotion_lvgg.h5\n",
            "Epoch 2/20\n",
            "1767/1767 [==============================] - 108s 61ms/step - loss: 1.8308 - acc: 0.2342 - val_loss: 1.8163 - val_acc: 0.2170\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.88296 to 1.81629, saving model to emotion_lvgg.h5\n",
            "Epoch 3/20\n",
            "1767/1767 [==============================] - 114s 65ms/step - loss: 1.8108 - acc: 0.2442 - val_loss: 1.7597 - val_acc: 0.2667\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.81629 to 1.75969, saving model to emotion_lvgg.h5\n",
            "Epoch 4/20\n",
            "1767/1767 [==============================] - 108s 61ms/step - loss: 1.8019 - acc: 0.2442 - val_loss: 1.8465 - val_acc: 0.2380\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.75969\n",
            "Epoch 5/20\n",
            "1767/1767 [==============================] - 107s 61ms/step - loss: 1.7658 - acc: 0.2651 - val_loss: 1.7279 - val_acc: 0.2841\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.75969 to 1.72791, saving model to emotion_lvgg.h5\n",
            "Epoch 6/20\n",
            "1767/1767 [==============================] - 109s 62ms/step - loss: 1.7474 - acc: 0.2746 - val_loss: 1.7800 - val_acc: 0.2981\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 1.72791\n",
            "Epoch 7/20\n",
            "1767/1767 [==============================] - 109s 62ms/step - loss: 1.7148 - acc: 0.2902 - val_loss: 1.6404 - val_acc: 0.3331\n",
            "\n",
            "Epoch 00007: val_loss improved from 1.72791 to 1.64041, saving model to emotion_lvgg.h5\n",
            "Epoch 8/20\n",
            "1767/1767 [==============================] - 117s 66ms/step - loss: 1.6327 - acc: 0.3442 - val_loss: 1.5904 - val_acc: 0.3956\n",
            "\n",
            "Epoch 00008: val_loss improved from 1.64041 to 1.59036, saving model to emotion_lvgg.h5\n",
            "Epoch 9/20\n",
            "1767/1767 [==============================] - 116s 65ms/step - loss: 1.5670 - acc: 0.3843 - val_loss: 1.7734 - val_acc: 0.3899\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.59036\n",
            "Epoch 10/20\n",
            "1767/1767 [==============================] - 107s 60ms/step - loss: 1.5068 - acc: 0.4182 - val_loss: 1.5973 - val_acc: 0.4457\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 1.59036\n",
            "Epoch 11/20\n",
            "1767/1767 [==============================] - 103s 58ms/step - loss: 1.4640 - acc: 0.4333 - val_loss: 1.5566 - val_acc: 0.4557\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.59036 to 1.55662, saving model to emotion_lvgg.h5\n",
            "Epoch 12/20\n",
            "1767/1767 [==============================] - 107s 60ms/step - loss: 1.4329 - acc: 0.4482 - val_loss: 1.5017 - val_acc: 0.4788\n",
            "\n",
            "Epoch 00012: val_loss improved from 1.55662 to 1.50175, saving model to emotion_lvgg.h5\n",
            "Epoch 13/20\n",
            "1767/1767 [==============================] - 106s 60ms/step - loss: 1.4116 - acc: 0.4589 - val_loss: 1.6365 - val_acc: 0.4622\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 1.50175\n",
            "Epoch 14/20\n",
            "1767/1767 [==============================] - 106s 60ms/step - loss: 1.3863 - acc: 0.4652 - val_loss: 1.4674 - val_acc: 0.4942\n",
            "\n",
            "Epoch 00014: val_loss improved from 1.50175 to 1.46741, saving model to emotion_lvgg.h5\n",
            "Epoch 15/20\n",
            "1767/1767 [==============================] - 107s 61ms/step - loss: 1.3628 - acc: 0.4758 - val_loss: 1.4927 - val_acc: 0.4779\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 1.46741\n",
            "Epoch 16/20\n",
            "1767/1767 [==============================] - 109s 62ms/step - loss: 1.3561 - acc: 0.4843 - val_loss: 1.4796 - val_acc: 0.4913\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 1.46741\n",
            "Epoch 17/20\n",
            "1767/1767 [==============================] - 106s 60ms/step - loss: 1.3279 - acc: 0.4948 - val_loss: 1.5084 - val_acc: 0.4876\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 1.46741\n",
            "\n",
            "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 18/20\n",
            "1767/1767 [==============================] - 106s 60ms/step - loss: 1.3076 - acc: 0.5034 - val_loss: 1.4273 - val_acc: 0.5061\n",
            "\n",
            "Epoch 00018: val_loss improved from 1.46741 to 1.42733, saving model to emotion_lvgg.h5\n",
            "Epoch 19/20\n",
            "1767/1767 [==============================] - 107s 60ms/step - loss: 1.2835 - acc: 0.5149 - val_loss: 1.3863 - val_acc: 0.5232\n",
            "\n",
            "Epoch 00019: val_loss improved from 1.42733 to 1.38631, saving model to emotion_lvgg.h5\n",
            "Epoch 20/20\n",
            "1767/1767 [==============================] - 106s 60ms/step - loss: 1.2730 - acc: 0.5201 - val_loss: 1.4447 - val_acc: 0.5084\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 1.38631\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}